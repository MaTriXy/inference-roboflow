[project]
name = "inference-exp"
version = "0.1.0"
description = "Experimental vresion of inference package which is supposed to evolve into inference 1.0"
readme = "README.md"
requires-python = ">=3.10,<3.13"
dependencies = [
  "numpy",
  "torch",
  "torchvision",
  "opencv-python>=4.8.1.78",
  "requests>=2.32.0,<3.0.0",
  "supervision==0.26.0rc7",  # just for now, as versions 0.25.x had issues with definitions of
  "backoff~=2.2.0",
  "transformers>=4.50.0,<5.0.0",
  "timm>=1.0.0,<2.0.0",
  "accelerate>=1.0.0,<2.0.0",
  "einops>=0.7.0,<1.0.0",
  "peft>=0.11.1,<0.16.0",
  "num2words~=0.5.14",
  "bitsandbytes>=0.42.0,<0.47.0; sys_platform != 'darwin'",
  "pyvips>=3.0.0,<4.0.0",
  "rf-clip==1.1",
  "python-doctr[torch]>=0.10.0,<=0.11.0",
]

[project.optional-dependencies]
torch-cpu = [
  "torch>2.0.0",
  "torchvision"
]
torch-cu118 = [
  "torch>2.0.0",
  "torchvision"
]
torch-cu126 = [
  "torch>=2.4.0",
  "torchvision"
]
torch-cu128 = [
  "torch>=2.4.0",
  "torchvision"
]
torch-jp6-cu128 = [
  "numpy<2.0.0",
  "torch>=2.0.0",
  "torchvision",
  "flash-attn==2.7.4.post1"
]
torch-jp5-cu114 = [
  "numpy<2.0.0",
  "torch>=2.0.0",
  "torchvision",
]
onnx-cpu = [
  "onnxruntime>=1.15.1,<1.23.0"
]
onnx-cu118 = [
  "onnxruntime-gpu>=1.15.1,<1.23.0; platform_system != 'darwin'"
]
onnx-cu12 = [
  "onnxruntime-gpu>=1.17.0,<1.23.0; platform_system != 'darwin'"
]
onnx-jp6-cu128 = [
  "numpy<2.0.0",
  "onnxruntime-gpu>=1.17.0,<1.23.0; platform_system == 'linux' and platform_machine == 'arm64' and python_version < '3.12' "
]
onnx-jp5-cu114 = [
  "numpy<2.0.0",
  "onnxruntime-gpu>=1.16.0,<1.23.0; platform_system == 'linux' and platform_machine == 'arm64' and python_version < '3.12' "
]
mediapipe = [
  "mediapipe>=0.9,<0.11"
]
grounding-dino = [
  "rf_groundingdino==0.2.0"
]
flash-attn = [
  "flash-attn==2.7.4.post1"
]

[tool.uv.sources]
torch = [
  { index = "torch-cpu", extra = "torch-cpu" },
  { index = "torch-cu118", extra = "torch-cu118" },
  { index = "torch-cu128", extra = "torch-cu128" },
  { index = "jp6-cu128", extra = "torch-jp6-cu128" },
  { index = "jp5-cu114", extra = "torch-jp5-cu114" },
]
torchvision = [
  { index = "torch-cpu", extra = "torch-cpu" },
  { index = "torch-cu118", extra = "torch-cu118" },
  { index = "torch-cu128", extra = "torch-cu128" },
  { index = "jp6-cu128", extra = "torch-jp6-cu128" },
  { index = "jp5-cu114", extra = "torch-jp5-cu114" },
]

onnxruntime-gpu = [
  { index = "onnx-cu118", extra = "onnx-cu118"},
  { index = "jp6-cu128", extra = "onnx-jp6-cu128" },
  { index = "jp5-cu114", extra = "onnx-jp5-cu114" },
]

flash-attn = [
   { index = "jp6-cu128", extra = "torch-jp6-cu128" },
]

[[tool.uv.index]]
name = "torch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "torch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[[tool.uv.index]]
name = "torch-cu118"
url = "https://download.pytorch.org/whl/cu118"
explicit = true

[[tool.uv.index]]
name = "jp6-cu128"
url = "https://pypi.jetson-ai-lab.dev/jp6/cu128/+simple"
explicit = true

[[tool.uv.index]]
name = "jp5-cu114"
url = "https://pypi.jetson-ai-lab.dev/jp5/cu114/+simple"
explicit = true

[[tool.uv.index]]
name = "onnx-cu118"
url = "https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-11/pypi/simple/"
explicit = true

[tool.setuptools]
packages = ["inference_exp"]

[tool.uv]
conflicts = [
  [
    { extra = "torch-cpu" },
    { extra = "torch-cu118" },
    { extra = "torch-cu126" },
    { extra = "torch-cu128" },
    { extra = "torch-jp6-cu128" },
    { extra = "torch-jp5-cu114" },
  ],
  [
    { extra = "onnx-cpu" },
    { extra = "onnx-cu118" },
    { extra = "onnx-cu12" },
    { extra = "onnx-jp6-cu128" },
    { extra = "onnx-jp5-cu114" },
  ],
]